{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AIML_Siamese Network_for_Image_Classification_on_IMFDB.ipynb","provenance":[],"collapsed_sections":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.0"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"sY8HfxCh_CSO"},"source":["# Advanced Certification in AIML\n","## A Program by IIIT-H and TalentSprint"]},{"cell_type":"markdown","metadata":{"id":"fRRaLUgO81jh"},"source":["## Learning Objectives"]},{"cell_type":"markdown","metadata":{"id":"vnrTzEfE9dgm"},"source":["At the end of the experiment, you will be able to:\n","\n","* Understand the challenges of facial recognition and IMFDB dataset\n","* Be able to use Siamese network to get similarity score between images\n","* Classify the IMFDB dataset using MLP by using the features obtained from Siamese Network\n"]},{"cell_type":"code","metadata":{"id":"8EMjDna1Kxgt","outputId":"6c2898d2-b5f4-4a41-ab84-422dadfe2d6f","colab":{"base_uri":"https://localhost:8080/","height":321}},"source":["#@title Experiment Explanation Video\n","from IPython.display import HTML\n","\n","HTML(\"\"\"<video width=\"500\" height=\"300\" controls>\n","  <source src=\"https://cdn.talentsprint.com/aiml/AIML_BATCH_HYD_7/Apr15/siamese_exp_walkthrough2.mp4\" type=\"video/mp4\">\n","</video>\n","\"\"\")"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<video width=\"500\" height=\"300\" controls>\n","  <source src=\"https://cdn.talentsprint.com/aiml/AIML_BATCH_HYD_7/Apr15/siamese_exp_walkthrough2.mp4\" type=\"video/mp4\">\n","</video>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"markdown","metadata":{"id":"K38V-8qoKqfq"},"source":["## Dataset"]},{"cell_type":"markdown","metadata":{"id":"fVdToIRQ9v-U"},"source":["### Description\n","\n","Indian Movie Face database (IMFDB) is a large unconstrained face database **consisting of 34512 images of 100 Indian actors collected from more than 100 videos.** \n","\n","* Faces in IMFDB are collected from Indian movie videos. Faces in movie videos arguably have large variations in scale, pose, expression, illumination, age, resolution, occlusion, and makeup.\n","* Videos collected from the last two decades contain large diversity in age variations compared to the images collected from Internet through a search query.\n","* Public figures often retain certain kind of appearance, dress patterns and expressions when appearing in public while movie videos offer greater variations.\n","* IMFDB is built by manual selection and cropping of video frames resulting in a large spectrum of poses develop efficient algorithms to handle pose.\n","\n","The image below contains few sample faces from the dataset:\n","\n","<img src = \"http://cvit.iiit.ac.in/projects/IMFDB/faces5.png\">\n","\n","Design guidelines followed in the database are:\n","\n","* **Selection of movies and actors**: To ensure diversity in appearance of actors, movies are selected from 5 Indian languages namely, Hindi, Telugu, Kannada, Malayalam, and Bengali. For each actor, movies are selected such that they give wide variations in age.\n","* **Selection of frames**: Often, a single frame is selected from a shot (scene) unless there is a another frame with significant difference with the previously selected frame. If there are multiple variations available in a shot, face with occlusion and pose variation, were preferred. Small faces that are difficult to recognize manually are not considered.\n","* **Cropping of faces**: Faces are cropped with a tight bounding box. In order to maintain consistency across images, we followed a heuristic of cropping the face from forehead to chin.\n","* **Annotation**: For every image, annotation is provided for following attributes:\n","\n","> \n","    Expressions     :  Anger, Happiness, Sadness, Surprise, Fear, Disgust\n","    Illumination    :  Bad, Medium, High\n","    Pose            :  Frontal, Left, Right, Up, Down \n","    Occlusion       :  Glasses, Beard, Ornaments, Hair, Hand\n","    Age             :  Child, Young, Middle and Old\n","    Makeup          :  Partial makeup, Over-makeup\n","    Gender          :  Male, Female\n","\n","*Source: Shankar Setty, Moula Husain, Parisa Beham, Jyothi Gudavalli, Menaka Kandasamy, Radhesyam Vaddi, Vidyagouri Hemadri, J C Karure, Raja Raju, Rajan, Vijay Kumar and C V Jawahar. \"Indian Movie Face Database: A Benchmark for Face Recognition Under Wide Variations\"\n","National Conference on Computer Vision, Pattern Recognition, Image Processing and Graphics (NCVPRIPG), 2013.*\n"]},{"cell_type":"markdown","metadata":{"id":"mtRlKzFg90Ec"},"source":["### Challenges\n","Face recognition systems yield satisfactory performance only under controlled scenarios and they degrade significantly when confronted with real-world scenarios.\n","\n","The real-world scenarios have unconstrained conditions such as illumination and pose variations, occlusion and expressions. These have to be considered while building the dataset and developing facial recognition systems."]},{"cell_type":"markdown","metadata":{"id":"ZRcNSMp6Lhjl"},"source":["## Domain Information"]},{"cell_type":"markdown","metadata":{"id":"ezpOrYbJ9jYM"},"source":["\n","Face recognition is one of the most popular applications of image analysis software today. \n","\n","For facial recognition software to identify unique facial features, it has to perform a number of tasks. \n","\n","**Face Recognition is the Last Step**\n","\n","* **Face detection**: first, the system has to identify the part of the image or the video that represents the face.\n","* **Pre-processing**: the data has to be transformed into a normalized. It is also often referred to as feature normalization.\n","* **Feature extraction**: the system has to extract meaningful data from the facial images, identifying the most relevant bits of data and ignoring all of the “noise.” It’s also referred to as encoding.\n","* **Face recognition**: the actual process of matching unique data features to each individual. \n"]},{"cell_type":"markdown","metadata":{"id":"ecb84yA2984p"},"source":["## AI/ML Technique"]},{"cell_type":"markdown","metadata":{"id":"I1P4t0xGAtGe"},"source":["### Siamese Network for Image Classification: \n","\n","In this experiment, we use IMFDB dataset for face recognition. Firstly, Siamese Network identifies similarities between images. Secondly, the various images can be classified by a Mulitlayer Perceptron (MLP).\n","\n","**Siamese Network** is a special type of neural network architecture. Instead of a model learning to classify its inputs, this neural networks learns to **differentiate** between two inputs. It learns the **similarity** between them.\n","\n","There are different classification models used for image classification. In this experiment, one shot classification model is used to perform the classification.\n","\n","**One Shot Classification models** require that you have just one training example of each class you want to predict on. The model is still trained on several instances, but they only have to be in the similar domain as your training example.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"VHbq_76zCFsx"},"source":["### Siamese Network Architecture\n","\n","It consists of two identical neural networks (sister networks), each taking one of the two input images. The last layers of the two networks are then fed to a contrastive loss function, which calculates the similarity between the two images. \n","\n","The image below helps us understand the above:\n","\n","<img src = \"https://cdn-images-1.medium.com/max/600/1*XzVUiq-3lYFtZEW3XfmKqg.jpeg\" width=\"400\" height = \"500\">\n","\n"]},{"cell_type":"markdown","metadata":{"id":"IuObTdeFBiGY"},"source":["Tons of data is available on the web (wikipedia, Google, Twitter, YouTube) that can be used to train an ML model.\n","One such source is Google Images. You enter a text query and Google Images show thousands of related images based on the query and text that are present on the web page with the related image."]},{"cell_type":"markdown","metadata":{"id":"f5Gw25muBlKU"},"source":["In this experiment we would crawl images from Google Images and try to use this as data for training."]},{"cell_type":"markdown","metadata":{"id":"GHi5xGcCBnA-"},"source":["1. Your task is to search for face images for 'AamairKhan', 'Rimisen', 'Kajol', 'KareenaKapoor','RishiKapoor', 'AmrishPuri', 'AnilKapoor', 'AnupamKher', 'BomanIrani', 'HrithikRoshan', 'KajalAgarwal', 'KatrinaKaif', 'Madhavan', 'MadhuriDixit', 'Umashri', 'Trisha'\n","\n","2. Refine your search to faces (Google Images -> enter query -> Tools -> Type -> Face). You could also use movies', ads' names as additional query (e.g., \"Aamir 3 idiots\", \"Boman Irani Khosla Ka Ghosla\", \"Katrina Slice ad\" etc.). The results are noisy but they are useful, and moreover, they are avaible in abundance and for free!\n","\n","    a. Example: https://www.google.co.in/search?client=firefox-b-ab&dcr=0&biw=1366&bih=628&tbs=itp%3Aface&tbm=isch&sa=1&ei=5gbIWtCjN4n2vgSCoqzYBw&q=biswa+kalyan+rath\n","\n","3. Then use a browser extensions to download all the results into a directory. In this way you, would get around 300-600 images for each class. Overall, you should collect atleast 10000 images.\n","    \n","    a. Firefox: https://addons.mozilla.org/en-US/firefox/addon/google-images-downloader/\n","    \n","    b. Chrome: https://chrome.google.com/webstore/detail/download-all-images/ifipmflagepipjokmbdecpmjbibjnakm/related?hl=en\n","    \n","4. **Without cleaning** use these images as your training data. Test you results on IMFDB test set.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"NOTDVdO_bH2d"},"source":["### Keywords:\n","\n","\n","\n","*   Siamese Network\n","* Contrastive Loss\n","*   Multilayer Perceptron\n","\n"]},{"cell_type":"markdown","metadata":{"id":"XwhjFQIDbudF"},"source":["##Expected time to complete the experiment is : 90 mins"]},{"cell_type":"markdown","metadata":{"id":"xbKgGvN8BtWG"},"source":["#### Run the Notebook on GPU"]},{"cell_type":"markdown","metadata":{"id":"lhoMm0ENB04C"},"source":["#### Setup Steps"]},{"cell_type":"code","metadata":{"id":"hFM_mHXUr0m5"},"source":["#@title Please enter your registration id to start: (e.g. P181900101) { run: \"auto\", display-mode: \"form\" }\n","Id = \"2001005\" #@param {type:\"string\"}\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z-UzlZNwr9vs"},"source":["#@title Please enter your password (normally your phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n","password = \"9160999072\" #@param {type:\"string\"}\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"ajgTM9bwHd5H","outputId":"c4810c27-6334-4e33-a7bc-2bf8df21096a","colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["#@title Run this cell to complete the setup for this Notebook\n","from IPython import get_ipython\n","\n","ipython = get_ipython()\n","  \n","notebook=\"M3W3_040_Siamese_C\" #name of the notebook\n","\n","def setup():\n","    ipython.magic(\"sx pip install torch==1.0.1 -f https://download.pytorch.org/whl/cu100/stable\")\n","    ipython.magic(\"sx pip install torchvision==0.2.1\")\n","    ipython.magic(\"sx wget https://cdn.talentsprint.com/aiml/Experiment_related_data/One_shot_Face_recognition.zip\")\n","    ipython.magic(\"sx unzip  One_shot_Face_recognition.zip\")\n","    ipython.magic(\"sx wget https://cdn.talentsprint.com/aiml/Experiment_related_data/data_loader.py\")\n","    ipython.magic(\"sx cd One_shot_Face_recognition\")\n","    print (\"Setup completed successfully\")\n","#  ipython.magic(\"sx pip3 install torch\")  \n","    from IPython.display import HTML, display\n","    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n","    print(\"Setup completed successfully\")\n","    return\n","\n","def submit_notebook():\n","    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n","    \n","    import requests, json, base64, datetime\n","\n","    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n","    if not submission_id:\n","      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n","      r = requests.post(url, data = data)\n","      r = json.loads(r.text)\n","\n","      if r[\"status\"] == \"Success\":\n","          return r[\"record_id\"]\n","      elif \"err\" in r:        \n","        print(r[\"err\"])\n","        return None        \n","      else:\n","        print (\"Something is wrong, the notebook will not be submitted for grading\")\n","        return None\n","    \n","    elif getAnswer() and getComplexity() and getAdditional() and getConcepts() and getWalkthrough() and getComments() and getInclassSupport() and getOnlineSupport():\n","      f = open(notebook + \".ipynb\", \"rb\")\n","      file_hash = base64.b64encode(f.read())\n","\n","      data = {\"complexity\" : Complexity, \"additional\" :Additional, \n","              \"concepts\" : Concepts, \"record_id\" : submission_id, \n","              \"answer\" : Answer, \"id\" : Id, \"file_hash\" : file_hash,\n","              \"notebook\" : notebook, \"feedback_walkthrough\":Walkthrough ,\n","              \"feedback_experiments_input\" : Comments,\n","              \"feedback_inclass_mentor\": Inclass_support,\n","              \"feedback_online_mentor\" : Online_support}\n","\n","      r = requests.post(url, data = data)\n","      r = json.loads(r.text)\n","      if \"err\" in r:        \n","        print(r[\"err\"])\n","        return None   \n","      else:\n","        print(\"Your submission is successful.\")\n","        print(\"Ref Id:\", submission_id)\n","        print(\"Date of submission: \", r[\"date\"])\n","        print(\"Time of submission: \", r[\"time\"])\n","        print(\"View your submissions: https://iiith-aiml.talentsprint.com/notebook_submissions\")\n","        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n","        return submission_id\n","    else: submission_id\n","    \n","\n","def getAdditional():\n","  try:\n","    if not Additional: \n","      raise NameError\n","    else:\n","      return Additional  \n","  except NameError:\n","    print (\"Please answer Additional Question\")\n","    return None\n","\n","def getComplexity():\n","  try:\n","    if not Complexity:\n","      raise NameError\n","    else:\n","      return Complexity\n","  except NameError:\n","    print (\"Please answer Complexity Question\")\n","    return None\n","  \n","def getConcepts():\n","  try:\n","    if not Concepts:\n","      raise NameError\n","    else:\n","      return Concepts\n","  except NameError:\n","    print (\"Please answer Concepts Question\")\n","    return None\n","  \n","  \n","def getWalkthrough():\n","  try:\n","    if not Walkthrough:\n","      raise NameError\n","    else:\n","      return Walkthrough\n","  except NameError:\n","    print (\"Please answer Walkthrough Question\")\n","    return None\n","  \n","def getComments():\n","  try:\n","    if not Comments:\n","      raise NameError\n","    else:\n","      return Comments\n","  except NameError:\n","    print (\"Please answer Comments Question\")\n","    return None\n","  \n","def getInclassSupport():\n","  try:\n","    if not Inclass_support:\n","      raise NameError\n","    else:\n","      return Inclass_support\n","  except NameError:\n","    print (\"Please answer Inclass support Question\")\n","    return None\n","  \n","  \n","def getOnlineSupport():\n","  try:\n","    if not Online_support:\n","      raise NameError\n","    else:\n","      return Online_support\n","  except NameError:\n","    print (\"Please answer Online support Question\")\n","    return None\n","\n","def getAnswer():\n","  try:\n","    if not Answer:\n","      raise NameError \n","    else: \n","      return Answer\n","  except NameError:\n","    print (\"Please answer Question\")\n","    return None\n","  \n","\n","def getId():\n","  try: \n","    return Id if Id else None\n","  except NameError:\n","    return None\n","\n","def getPassword():\n","  try:\n","    return password if password else None\n","  except NameError:\n","    return None\n","\n","submission_id = None\n","### Setup \n","if getPassword() and getId():\n","  submission_id = submit_notebook()\n","  if submission_id:\n","    setup() \n","else:\n","  print (\"Please complete Id and Password cells before running setup\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Setup completed successfully\n"],"name":"stdout"},{"output_type":"display_data","data":{"text/html":["<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId=2001005&recordId=17480\"></script>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Setup completed successfully\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6iyFFSx2_YVv"},"source":["\n","from IPython import get_ipython\n","\n","ipython = get_ipython()\n","  \n","notebook=\"M3W3_040_Siamese_C\" #name of the notebook\n","\n","def setup():\n","    ipython.magic(\"sx pip install torch==1.0.1 -f https://download.pytorch.org/whl/cu100/stable\")\n","    ipython.magic(\"sx pip install torchvision==0.2.1\")\n","    ipython.magic(\"sx wget https://cdn.talentsprint.com/aiml/Experiment_related_data/One_shot_Face_recognition.zip\")\n","    ipython.magic(\"sx unzip  One_shot_Face_recognition.zip\")\n","    ipython.magic(\"sx wget https://cdn.talentsprint.com/aiml/Experiment_related_data/data_loader.py\")\n","    ipython.magic(\"sx cd One_shot_Face_recognition\")\n","    print (\"Setup completed successfully\")\n","#  ipython.magic(\"sx pip3 install torch\")  \n","    from IPython.display import HTML, display\n","    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n","    print(\"Setup completed successfully\")\n","    return\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JSVGip5-LmQl","outputId":"80fd260c-15e3-4a5a-8642-53f2e9bfe08e","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["%cd /content/\n","from data_loader import custom_data_loader"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pFHSsPIuF04j","outputId":"9f6af903-cfab-4951-f7c3-2e48510ad969","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["%cd /content/One_shot_Face_recognition"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/One_shot_Face_recognition\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"aq9clcuMCRVy"},"source":["#### Importing the Required Packages"]},{"cell_type":"code","metadata":{"id":"APSGlS_s_CSU"},"source":["# Importing pytorch packages\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torch.backends.cudnn as cudnn\n","import torchvision\n","import torchvision.transforms as transforms\n","from torch.autograd import Variable\n","# Importing config.py file\n","import config as cf\n","from utils import *\n","from light_cnn import LightCNN_9Layers #, LightCNN_29Layers, LightCNN_29Layers_v2\n","#from resnet import resnet18\n","from siamese_data_loader import *\n","from contrastive import *   ### implementation of contrastive loss\n","## Importing python packages\n","import os\n","import sys\n","import time\n","import datetime\n","import numpy as np\n","import math\n","import matplotlib.pyplot as plt\n","import pickle"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oia-EoIHCjo0"},"source":["#### Loading the data "]},{"cell_type":"code","metadata":{"id":"LWG1XpBV_CSi","outputId":"4b678ab3-c178-4329-ecfb-edc9437efc64","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["img_root = cf.data_dir+'IMFDB_final/'\n","\n","train_list_file = cf.data_dir+'IMFDB_train_sorted.txt'   #### 5000 images for training\n","val_list_file = cf.data_dir+'IMFDB_test_sorted.txt'      #### 1095 images for validation\n","\n","\n","train_image_list = [line.rstrip('\\n') for line in open(train_list_file)]\n","val_image_list = [line.rstrip('\\n') for line in open(val_list_file)]\n","\n","print(len(train_image_list), len(val_image_list))\n","\n","### Notice a new data loader for siamese networks. This gives the image pairs (image_1, image_2) and a label as input to the siamese networks.\n","### see siamese_data_loader.py for details\n","\n","trainloader = torch.utils.data.DataLoader(siamese_data_loader(img_root = img_root, image_list = train_list_file, crop=False,\n","                                                            resize = True, resize_shape=[128,128]), \n","batch_size=32, num_workers=1, shuffle = False, pin_memory=False)\n","\n","testloader = torch.utils.data.DataLoader(siamese_data_loader(img_root = img_root, image_list = val_list_file, crop=False, mirror=False, \n","                                                          resize = True, resize_shape=[128,128]), \n","                                          batch_size=10, num_workers=1, shuffle = False, pin_memory=False)\n","\n","\n","classes = ['AamairKhan', 'Rimisen', 'Kajol', 'KareenaKapoor','RishiKapoor', 'AmrishPuri', 'AnilKapoor', 'AnupamKher', 'BomanIrani', 'HrithikRoshan', 'KajalAgarwal', 'KatrinaKaif', 'Madhavan', 'MadhuriDixit', 'Umashri', 'Trisha']"],"execution_count":null,"outputs":[{"output_type":"stream","text":["5000 1095\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"u2Aee8StCokK"},"source":["#### Command to check whether GPU is enabled or not\n"]},{"cell_type":"code","metadata":{"id":"e0_YCzNqeLyB"},"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VvlieheJ_CSy"},"source":["\n","#Intilizaing the loss value as high value\n","best_loss = 99999999\n","\n","num_classes = 16"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y4NzPR-weLyH"},"source":["from torchvision import models "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"40tFVFoWeLyM"},"source":["feature_net = LightCNN_9Layers()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C2kay4SoG6dd","outputId":"a8959937-2c7f-4d43-9e1c-ad50e5165c57","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["print(cf.data_dir)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["data/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ort2nuVUeLyS","outputId":"c29b33b6-899d-4433-f44f-5aaaa5d5e684","colab":{"base_uri":"https://localhost:8080/","height":184}},"source":["feature_net = torch.load(cf.data_dir+'lightCNN_51_checkpoint.pth')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/serialization.py:435: SourceChangeWarning: source code of class 'torch.nn.modules.container.Sequential' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n","  warnings.warn(msg, SourceChangeWarning)\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:435: SourceChangeWarning: source code of class 'torch.nn.modules.conv.Conv2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n","  warnings.warn(msg, SourceChangeWarning)\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:435: SourceChangeWarning: source code of class 'torch.nn.modules.pooling.MaxPool2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n","  warnings.warn(msg, SourceChangeWarning)\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:435: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n","  warnings.warn(msg, SourceChangeWarning)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"I6-oKdUGDCe8","outputId":"47379a01-843a-4d3b-8a33-9267fbbc5a62","colab":{"base_uri":"https://localhost:8080/","height":90}},"source":["%ls"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[0m\u001b[01;32mconfig.py\u001b[0m*            IMFDB_train_label.txt           MISC.ipynb\n","\u001b[01;32mcontrastive.py\u001b[0m*       \u001b[01;32mLab11-Experiment2_3_cpu.ipynb\u001b[0m*  \u001b[01;34m__pycache__\u001b[0m/\n","\u001b[01;34mdata\u001b[0m/                 light_cnn.py                    \u001b[01;32msiamese_data_loader.py\u001b[0m*\n","IMFDB_test_label.txt  LightCNN_train.ipynb            \u001b[01;32mutils.py\u001b[0m*\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zlkvrFIeeLyW","outputId":"5fd9115a-3bb2-47bb-fa44-6ed3f87a207b","colab":{"base_uri":"https://localhost:8080/","height":880}},"source":["feature_net"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["network_9layers(\n","  (features): Sequential(\n","    (0): mfm(\n","      (filter): Conv2d(1, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n","    )\n","    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n","    (2): group(\n","      (conv_a): mfm(\n","        (filter): Conv2d(48, 96, kernel_size=(1, 1), stride=(1, 1))\n","      )\n","      (conv): mfm(\n","        (filter): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      )\n","    )\n","    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n","    (4): group(\n","      (conv_a): mfm(\n","        (filter): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1))\n","      )\n","      (conv): mfm(\n","        (filter): Conv2d(96, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      )\n","    )\n","    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n","    (6): group(\n","      (conv_a): mfm(\n","        (filter): Conv2d(192, 384, kernel_size=(1, 1), stride=(1, 1))\n","      )\n","      (conv): mfm(\n","        (filter): Conv2d(192, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      )\n","    )\n","    (7): group(\n","      (conv_a): mfm(\n","        (filter): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n","      )\n","      (conv): mfm(\n","        (filter): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      )\n","    )\n","    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=True)\n","  )\n","  (fc1): mfm(\n","    (filter): Linear(in_features=8192, out_features=512, bias=True)\n","  )\n","  (fc2): Linear(in_features=256, out_features=16, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"id":"yUJKM-V5_CS-","outputId":"8931b183-1d94-43a0-a0d4-b127e843652d","colab":{"base_uri":"https://localhost:8080/","height":184}},"source":["feature_net = LightCNN_9Layers()   ### creates an object of this network architecture\n","feature_net = torch.load(cf.data_dir+'lightCNN_51_checkpoint.pth')\n","\n","\n","layers_to_remove = ['fc2']\n","for layers_ in layers_to_remove:        \n","    del(feature_net._modules[layers_])\n","    \n","classifier = nn.Sequential(nn.Linear(256, 64), nn.BatchNorm1d(64), nn.ReLU(),\n","                           nn.Linear(64, 32), nn.BatchNorm1d(32), nn.ReLU(),\n","                           nn.Linear(32, num_classes))\n","\n","feature_net.fc2 = nn.Sequential(nn.Linear(256, 16))\n","feature_net = feature_net.to(device)\n","classifier =  classifier.to(device)\n","    \n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/serialization.py:435: SourceChangeWarning: source code of class 'torch.nn.modules.container.Sequential' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n","  warnings.warn(msg, SourceChangeWarning)\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:435: SourceChangeWarning: source code of class 'torch.nn.modules.conv.Conv2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n","  warnings.warn(msg, SourceChangeWarning)\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:435: SourceChangeWarning: source code of class 'torch.nn.modules.pooling.MaxPool2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n","  warnings.warn(msg, SourceChangeWarning)\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:435: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n","  warnings.warn(msg, SourceChangeWarning)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"Dvh9E5z0_CTG"},"source":["### Intiliazing the loss\n","criterion = nn.CrossEntropyLoss()\n","siamese_loss = contrastive_loss()   ### Notice a new loss. contrastive.py shows how to compute contrastive loss."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JFloT0WAeLyw"},"source":["criterion = criterion.to(device)\n","siamese_loss = siamese_loss.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lOmg3Ypb_CTO"},"source":["#### Lets train the siamese networks. The objective is images from same class (+ pair, label = 0) should have similar feature and images from different classes (- pair, label = 1) should have different features. Instead of having two physical networks sharing the weights, in implementation we have only one network and first pass image_1 (to get its feature) and then pass image_2 (to get its feature) through the same network. We then compute the contrastive loss on these feature pairs from input image pairs. This saves a lot of memory."]},{"cell_type":"code","metadata":{"id":"isK--8bS_CTS"},"source":["def train(epoch):\n","    print('\\nEpoch: %d' % epoch)\n","    feature_net.train()\n","    train_loss = 0\n","    correct = 0\n","    total = 1\n","    for batch_idx, (inputs_1, inputs_2, targets) in enumerate(trainloader):\n","        inputs_1, inputs_2, targets = inputs_1.to(device), inputs_2.to(device), targets.to(device)\n","        optimizer.zero_grad()\n","        #inputs_1, inputs_2, targets = inputs_1), Variable(inputs_2), Variable(targets)\n","        features_1 = feature_net(inputs_1)[1]     ### get feature for image_1\n","        features_2 = feature_net(inputs_2)[1]      ### get feature for image_2\n","        \n","        loss = siamese_loss(features_1, features_2, targets.float())   ### compute the contrastive loss, computes the similarity between the features.\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.item()\n","        #print(1)\n","        \n","        progress_bar(batch_idx, len(trainloader), 'Loss: %.3f '% (train_loss/(batch_idx+1)))\n","        \n","    train_loss_file.write('%d %.3f %.3f\\n' %(epoch, train_loss/len(trainloader), 100.*correct/total))\n","        #print(1)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qj7vLS23C_SS"},"source":["#### Function to test\n","\n"]},{"cell_type":"code","metadata":{"id":"zMwHtczA_CTc"},"source":["def test(epoch):\n","    global best_loss\n","    feature_net.eval()\n","    test_loss = 0\n","    correct = 0\n","    total = 1\n","    for batch_idx, (inputs_1, inputs_2, targets) in enumerate(testloader):\n","        inputs_1, inputs_2, targets = inputs_1.to(device), inputs_2.to(device), targets.to(device)\n","        optimizer.zero_grad()\n","        #inputs_1, inputs_2, targets = Variable(inputs_1), Variable(inputs_2), Variable(targets)\n","        features_1 = feature_net(inputs_1)[1]     ### get feature for image_1\n","        features_2 = feature_net(inputs_2)[1]      ### get feature for image_2      \n","        \n","        loss = siamese_loss(features_1, features_2, targets.float())\n","        test_loss += loss.item()\n","        \n","        progress_bar(batch_idx, len(testloader), 'Loss: %.3f '\n","                         % (test_loss/(batch_idx+1)))\n","        \n","    val_loss_file.write('%d %.3f %.3f\\n' %(epoch,  test_loss/len(testloader), 100.*correct/total))\n","\n","    # Save checkpoint.\n","    losss = test_loss/len(testloader)\n","    if  losss < best_loss:   ### save model with the best loss so far\n","        print('Saving..') \n","        state = {\n","            'net': feature_net\n","        }\n","        if not os.path.isdir(cf.data_dir+'checkpoint'):\n","            os.mkdir(cf.data_dir+'checkpoint')\n","        torch.save(state, cf.data_dir+'checkpoint/siamese_ckpt.t7')\n","        best_loss = losss\n","    \n","    return test_loss/len(testloader)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3k0q1BOVDD86"},"source":["#### Creating the files to store train and validation data loss values"]},{"cell_type":"code","metadata":{"id":"KhnUSGoC_CTk"},"source":["experiment = 'siamese_IMFDB/'\n","train_loss_file = open(cf.data_dir+experiment+\"train_loss.txt\", \"w\")\n","val_loss_file = open(cf.data_dir+experiment+\"val_loss.txt\", \"w\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xKSAHBYR_CT0","outputId":"00b22390-57f7-42b7-8d60-24db8dc56672","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["feature_net = feature_net.to(device)\n","optimizer = optim.Adam(feature_net.parameters(), lr=1e-3)\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=2, verbose=True)   #### dynamic LR scheduler\n","for epoch in range(0, 10):\n","    train(epoch)\n","    test_loss = test(epoch)\n","    scheduler.step(test_loss)\n","    print(\"Test Loss: \", test_loss)\n","train_loss_file.close()\n","val_loss_file.close()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","Epoch: 0\n"," [==================================>] | Loss: 456169.096                       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 157/157 \n"," [==================================>] | Loss: 13.443                           \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 110/110 \n","Saving..\n","Test Loss:  13.44291646047072\n","\n","Epoch: 1\n"," [==================================>] | Loss: 118.350                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 157/157 \n"," [==================================>] | Loss: 7.679                            \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 110/110 \n","Saving..\n","Test Loss:  7.679208109595558\n","\n","Epoch: 2\n"," [==================================>] | Loss: 74.410                           \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 157/157 \n"," [==================================>] | Loss: 5.593                            \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 110/110 \n","Saving..\n","Test Loss:  5.593335193937475\n","\n","Epoch: 3\n"," [==================================>] | Loss: 55.074                           \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 157/157 \n"," [==================================>] | Loss: 4.345                            \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 110/110 \n","Saving..\n","Test Loss:  4.344905295155265\n","\n","Epoch: 4\n"," [==================================>] | Loss: 47.741                           \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 157/157 \n"," [==================================>] | Loss: 3.921                            \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 110/110 \n","Saving..\n","Test Loss:  3.920588433742523\n","\n","Epoch: 5\n"," [==================================>] | Loss: 41.622                           \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 157/157 \n"," [==================================>] | Loss: 3.089                            \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 110/110 \n","Saving..\n","Test Loss:  3.0893301102248105\n","\n","Epoch: 6\n"," [==================================>] | Loss: 39.564                           \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 157/157 \n"," [==================================>] | Loss: 2.606                            \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 110/110 \n","Saving..\n","Test Loss:  2.6060165936296635\n","\n","Epoch: 7\n"," [==================================>] | Loss: 30.683                           \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 157/157 \n"," [==================================>] | Loss: 2.251                            \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 110/110 \n","Saving..\n","Test Loss:  2.2511927328326484\n","\n","Epoch: 8\n"," [==================================>] | Loss: 24.576                           \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 157/157 \n"," [==================================>] | Loss: 1.968                            \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 110/110 \n","Saving..\n","Test Loss:  1.9680872028524226\n","\n","Epoch: 9\n"," [==================================>] | Loss: 23.538                           \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 157/157 \n"," [==================================>] | Loss: 1.771                            \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 110/110 \n","Saving..\n","Test Loss:  1.771210396831686\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xYMrwcsA_CUG"},"source":["### After training we load the model that performed the best on validation data (avoid picking overfitted model)\n","### we will use the base pre-trained network for feature extraction only. This feature is used to train an MLP classifier.\n","\n","feature_net = torch.load(cf.data_dir+'checkpoint/siamese_ckpt.t7')['net'].eval()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Kf4WaSVdU5zw","outputId":"04d5c9f0-b7c1-47ca-fceb-bac4cc07caf7","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["os.path.exists('/content/MyDrive')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["False"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"markdown","metadata":{"id":"48su1H5b_CUO"},"source":["#### Lets see how well does the siamese detect an imposter. We check whether image_2 is same individual as image_1 or an imposter. We do this by computing dissimilarity score between features."]},{"cell_type":"code","metadata":{"id":"fSo3BA0a_CUQ"},"source":["testloader = torch.utils.data.DataLoader(siamese_data_loader(img_root = img_root, image_list = val_list_file, crop=False, mirror=False, \n","                                                           resize = True, resize_shape=[128,128]), \n","                                           batch_size=1, num_workers=1, shuffle = False, pin_memory=False)\n","\n","lab = ['same', 'imposter']\n","with torch.no_grad():\n","  for batch_idx, (inputs_1, inputs_2, targets) in enumerate(testloader):\n","      if batch_idx%10 == 0 or int(targets)==0:      ### show every tenth image or if its the same individual\n","\n","          inputs_1, inputs_2, targets = inputs_1.to(device), inputs_2.to(device), targets.to(device)\n","          optimizer.zero_grad()\n","          #inputs_1, inputs_2, targets = inputs_1), Variable(inputs_2), Variable(targets)\n","          features_1 = feature_net(inputs_1)[1]     ### get feature for image_1\n","          features_2 = feature_net(inputs_2)[1]      ### get feature for image_2\n","\n","          dissimilarity = torch.nn.functional.cosine_similarity(features_1, features_2).item()\n","          img = np.concatenate((inputs_1.data.cpu().numpy()[0][0], inputs_2.data.cpu().numpy()[0][0]), axis = 1)\n","          plt.imshow(img, cmap='gray')\n","          plt.text(100,20,str(dissimilarity), fontsize=24, color='r')     ### similarity score\n","          plt.text(100,40,lab[int(targets.data[0])], fontsize=24, color='r')   ### ground truth\n","          plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-9di4sNM_CUc"},"source":["### Now we use this network for feature extraction and train an MLP classifier. Feature_net is not updated/train/tweak after this. We only train the MLP classifier."]},{"cell_type":"code","metadata":{"id":"FTC3qFNJ_CUe","outputId":"4bf6b300-c52d-4877-b664-32b91ef8632e","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["\n","\n","train_list_file = cf.data_dir+'IMFDB_train.txt'   #### 5000 images for training\n","val_list_file = cf.data_dir+'IMFDB_test.txt'      #### 1095 images for validation\n","\n","\n","train_image_list = [line.rstrip('\\n') for line in open(train_list_file)]\n","val_image_list = [line.rstrip('\\n') for line in open(val_list_file)]\n","\n","print(len(train_image_list), len(val_image_list))\n","\n","trainloader = torch.utils.data.DataLoader(custom_data_loader(img_root = img_root, image_list = train_list_file, crop=False,\n","                                                             resize = True, resize_shape=[128,128]), \n","                                          batch_size=32, num_workers=16, shuffle = True, pin_memory=False)\n","\n","testloader = torch.utils.data.DataLoader(custom_data_loader(img_root = img_root, image_list = val_list_file, crop=False, mirror=False, \n","                                                           resize = True, resize_shape=[128,128]), \n","                                         batch_size=10, num_workers=5, shuffle = False, pin_memory=False)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["5000 1095\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-tfWqQZB_CUm"},"source":["def train_classifier(epoch):\n","    print('\\nEpoch: %d' % epoch)\n","    classifier.train()\n","    train_loss = 0\n","    correct = 0\n","    total = 0\n","    for batch_idx, (inputs, targets) in enumerate(trainloader):\n","        inputs, targets = inputs.to(device), targets.to(device)\n","        optimizer.zero_grad()\n","        #inputs, targets = Variable(inputs), Variable(targets)\n","        features = feature_net(inputs)[1]      \n","        \n","        \n","        outputs = classifier(features)\n","        size_ = outputs.size()\n","        outputs_ = outputs.view(size_[0], num_classes)\n","        loss = criterion(outputs_, targets)\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.data\n","        _, predicted = torch.max(outputs_.data, 1)\n","        total += targets.size(0)\n","        correct += predicted.eq(targets.data).cpu().sum()\n","        \n","        progress_bar(batch_idx, len(trainloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n","                         % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n","        \n","    train_loss_file.write('%d %.3f %.3f\\n' %(epoch, train_loss/len(trainloader), 100.*correct/total))\n","    \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"srGe92K5_CUu"},"source":["def test_classifier(epoch):\n","    global best_acc\n","    classifier.eval()\n","    test_loss = 0\n","    correct = 0\n","    total = 0\n","    for batch_idx, (inputs, targets) in enumerate(testloader):\n","        #if device:\n","        inputs, targets = inputs.to(device), targets.to(device)\n","        #inputs, targets = Variable(inputs, volatile=True), Variable(targets)\n","        features = feature_net(inputs)[1]\n","        \n","        outputs = classifier(features)\n","        size_ = outputs.size()\n","        outputs_ = outputs.view(size_[0], num_classes)\n","        loss = criterion(outputs_, targets)\n","\n","        test_loss += loss.item()\n","        _, predicted = torch.max(outputs_.data, 1)\n","        total += targets.size(0)\n","        correct += predicted.eq(targets.data).cpu().sum()\n","        \n","        progress_bar(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n","                         % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n","        \n","    val_loss_file.write('%d %.3f %.3f\\n' %(epoch,  test_loss/len(testloader), 100.*correct/total))\n","\n","    # Save checkpoint.\n","    acc = 100.*correct/total\n","    if acc > best_acc:\n","        print('Saving..')\n","        state = {\n","            'net': classifier,\n","            'acc': acc,\n","            'epoch': epoch,\n","        }\n","        if not os.path.isdir(cf.data_dir+'checkpoint'):\n","            os.mkdir(cf.data_dir+'checkpoint')\n","        torch.save(state, cf.data_dir+'checkpoint/checkpoint_ckpt.t7')\n","        best_acc = acc\n","    \n","    return test_loss/len(testloader)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bO-t7kPP_CU0"},"source":["best_acc = 0\n","experiment = 'siamese_IMFDB'\n","train_loss_file = open(cf.data_dir+experiment+\"train_loss.txt\", \"w\")\n","val_loss_file = open(cf.data_dir+experiment+\"val_loss.txt\", \"w\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_e3YpfZz_CU4","outputId":"650b64bb-03c8-482b-a3f1-0b2397c9d290","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["optimizer = optim.Adam(classifier.parameters(), lr=0.001)\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=2, verbose=True)   #### dynamic LR scheduler\n","for epoch in range(0, 30):\n","    train_classifier(epoch)\n","    test_loss = test_classifier(epoch)\n","    scheduler.step(test_loss)\n","    \n","train_loss_file.close()\n","val_loss_file.close()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","Epoch: 0\n"," [==================================>] | Loss: 2.653 | Acc: 15.000% (754/5000)  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 157/157 \n"," [==================================>] | Loss: 2.513 | Acc: 23.000% (254/1095)  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 110/110 \n","Saving..\n","\n","Epoch: 1\n"," [==================================>] | Loss: 2.338 | Acc: 29.000% (1477/5000) \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 157/157 \n"," [==================================>] | Loss: 2.208 | Acc: 32.000% (354/1095)  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 110/110 \n","Saving..\n","\n","Epoch: 2\n"," [==================================>] | Loss: 2.041 | Acc: 38.000% (1907/5000) \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 157/157 \n"," [==================================>] | Loss: 2.017 | Acc: 36.000% (404/1095)  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 110/110 \n","Saving..\n","\n","Epoch: 3\n"," [==================================>] | Loss: 1.820 | Acc: 44.000% (2247/5000) \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 157/157 \n"," [==================================>] | Loss: 1.892 | Acc: 39.000% (429/1095)  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 110/110 \n","Saving..\n","\n","Epoch: 4\n"," [==================================>] | Loss: 1.716 | Acc: 46.000% (2339/5000) \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 157/157 \n"," [==================================>] | Loss: 1.834 | Acc: 42.000% (462/1095)  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 110/110 \n","Saving..\n","\n","Epoch: 5\n"," [==================================>] | Loss: 1.599 | Acc: 51.000% (2569/5000) \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 157/157 \n"," [==================================>] | Loss: 1.821 | Acc: 41.000% (454/1095)  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 110/110 \n","\n","Epoch: 6\n"," [==================================>] | Loss: 1.509 | Acc: 53.000% (2691/5000) \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 157/157 \n"," [==================================>] | Loss: 1.804 | Acc: 43.000% (480/1095)  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 110/110 \n","Saving..\n","\n","Epoch: 7\n"," [==================================>] | Loss: 1.431 | Acc: 54.000% (2745/5000) \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 157/157 \n"," [==================================>] | Loss: 1.774 | Acc: 43.000% (480/1095)  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 110/110 \n","\n","Epoch: 8\n"," [==================================>] | Loss: 1.412 | Acc: 56.000% (2827/5000) \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 157/157 \n"," [==================================>] | Loss: 1.751 | Acc: 45.000% (500/1095)  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 110/110 \n","Saving..\n","\n","Epoch: 9\n"," [==================================>] | Loss: 1.336 | Acc: 57.000% (2881/5000) \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 157/157 \n"," [==================================>] | Loss: 1.754 | Acc: 43.000% (480/1095)  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 110/110 \n","\n","Epoch: 10\n"," [==================================>] | Loss: 1.293 | Acc: 59.000% (2998/5000) \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 157/157 \n"," [==================================>] | Loss: 1.725 | Acc: 46.000% (510/1095)  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 110/110 \n","Saving..\n","\n","Epoch: 11\n"," [==================================>] | Loss: 1.241 | Acc: 60.000% (3046/5000) \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 157/157 \n"," [==================================>] | Loss: 1.752 | Acc: 46.000% (510/1095)  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 110/110 \n","\n","Epoch: 12\n"," [==================================>] | Loss: 1.247 | Acc: 61.000% (3055/5000) \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 157/157 \n"," [==================================>] | Loss: 1.740 | Acc: 46.000% (513/1095)  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 110/110 \n","\n","Epoch: 13\n"," [==================================>] | Loss: 1.204 | Acc: 62.000% (3100/5000) \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 157/157 \n"," [==================================>] | Loss: 1.758 | Acc: 46.000% (509/1095)  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 110/110 \n","Epoch    13: reducing learning rate of group 0 to 5.0000e-04.\n","\n","Epoch: 14\n"," [==================================>] | Loss: 1.132 | Acc: 64.000% (3224/5000) \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 157/157 \n"," [==================================>] | Loss: 1.748 | Acc: 47.000% (525/1095)  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 110/110 \n","Saving..\n","\n","Epoch: 15\n"," [==================================>] | Loss: 1.079 | Acc: 66.000% (3339/5000) \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 157/157 \n"," [==================================>] | Loss: 1.732 | Acc: 48.000% (528/1095)  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 110/110 \n","Saving..\n","\n","Epoch: 16\n"," [==================================>] | Loss: 1.069 | Acc: 65.000% (3281/5000) \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 157/157 \n"," [==================================>] | Loss: 1.761 | Acc: 46.000% (514/1095)  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 110/110 \n","Epoch    16: reducing learning rate of group 0 to 2.5000e-04.\n","\n","Epoch: 17\n"," [==================================>] | Loss: 1.021 | Acc: 68.000% (3426/5000) \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 157/157 \n"," [==================================>] | Loss: 1.749 | Acc: 46.000% (513/1095)  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 110/110 \n","\n","Epoch: 18\n"," [==================================>] | Loss: 0.994 | Acc: 68.000% (3448/5000) \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 157/157 \n"," [==================================>] | Loss: 1.750 | Acc: 47.000% (517/1095)  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 110/110 \n","\n","Epoch: 19\n"," [==================================>] | Loss: 1.000 | Acc: 69.000% (3457/5000) \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 157/157 \n"," [==================================>] | Loss: 1.751 | Acc: 47.000% (522/1095)  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 110/110 \n","Epoch    19: reducing learning rate of group 0 to 1.2500e-04.\n","\n","Epoch: 20\n"," [==================================>] | Loss: 0.983 | Acc: 69.000% (3484/5000) \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 157/157 \n"," [==================================>] | Loss: 1.756 | Acc: 48.000% (532/1095)  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 110/110 \n","\n","Epoch: 21\n"," [==================================>] | Loss: 0.944 | Acc: 70.000% (3530/5000) \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 157/157 \n"," [==================================>] | Loss: 1.760 | Acc: 47.000% (517/1095)  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 110/110 \n","\n","Epoch: 22\n"," [==================================>] | Loss: 0.954 | Acc: 70.000% (3522/5000) \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 157/157 \n"," [==================================>] | Loss: 1.757 | Acc: 48.000% (526/1095)  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 110/110 \n","Epoch    22: reducing learning rate of group 0 to 6.2500e-05.\n","\n","Epoch: 23\n"," [==================================>] | Loss: 0.923 | Acc: 71.000% (3574/5000) \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 157/157 \n"," [==================================>] | Loss: 1.770 | Acc: 47.000% (520/1095)  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 110/110 \n","\n","Epoch: 24\n"," [==================================>] | Loss: 0.935 | Acc: 71.000% (3592/5000) \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 157/157 \n"," [==================================>] | Loss: 1.762 | Acc: 46.000% (512/1095)  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 110/110 \n","\n","Epoch: 25\n"," [==================================>] | Loss: 0.933 | Acc: 71.000% (3579/5000) \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 157/157 \n"," [==================================>] | Loss: 1.750 | Acc: 47.000% (521/1095)  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 110/110 \n","Epoch    25: reducing learning rate of group 0 to 3.1250e-05.\n","\n","Epoch: 26\n"," [==================================>] | Loss: 0.904 | Acc: 71.000% (3599/5000) \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 157/157 \n"," [==================================>] | Loss: 1.761 | Acc: 47.000% (520/1095)  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 110/110 \n","\n","Epoch: 27\n"," [==================================>] | Loss: 0.910 | Acc: 71.000% (3596/5000) \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 157/157 \n"," [==================================>] | Loss: 1.775 | Acc: 47.000% (519/1095)  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 110/110 \n","\n","Epoch: 28\n"," [==================================>] | Loss: 0.908 | Acc: 72.000% (3623/5000) \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 157/157 \n"," [==================================>] | Loss: 1.755 | Acc: 47.000% (520/1095)  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 110/110 \n","Epoch    28: reducing learning rate of group 0 to 1.5625e-05.\n","\n","Epoch: 29\n"," [==================================>] | Loss: 0.908 | Acc: 72.000% (3622/5000) \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 157/157 \n"," [==================================>] | Loss: 1.775 | Acc: 48.000% (526/1095)  \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 110/110 \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_zOf7cwQ_CVG"},"source":["def eval():\n","    feature_net.eval()\n","    classifier.eval()\n","    \n","    testloader = torch.utils.data.DataLoader(custom_data_loader(img_root = img_root, image_list = val_list_file, crop=False, mirror=False, \n","                                                           resize = True, resize_shape=[128,128]), \n","                                           batch_size=1, num_workers=1, shuffle = False, pin_memory=False)\n","    correct = 0\n","    total = 0\n","    conf_mat = np.zeros((num_classes, num_classes))\n","    total_ = 1e-12+np.zeros((num_classes))\n","    wrong_predictions = []\n","    for batch_idx, (inputs, targets) in enumerate(testloader):\n","        #if use_cuda:\n","        inputs, targets = inputs.to(device), targets.to(device)\n","        #inputs, targets = Variable(inputs, volatile=True), Variable(targets)\n","        features = feature_net.features(inputs).detach()\n","        features = features.view(1,-1)\n","        #print(features.size())        \n","        outputs = classifier(features)\n","        size_ = outputs.size()\n","        outputs_ = outputs.view(size_[0], num_classes)\n","        _, predicted = torch.max(outputs_.data, 1)\n","        total += targets.size(0)\n","        correct += predicted.eq(targets.data).cpu().sum()\n","        prediction = predicted.cpu().numpy()[0]\n","        targets = targets.data.cpu().numpy()[0]\n","        total_[targets] +=1\n","        conf_mat[predicted, targets] +=1\n","        \n","        if prediction != targets:\n","            wrong_predictions += [[inputs, prediction, targets]]\n","        \n","    for k in range(num_classes):\n","        conf_mat[:,k] /= total_[k]\n","    return conf_mat, 100.*correct/total, wrong_predictions\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xu5-2sTarNfA"},"source":["classifier = nn.Sequential(nn.Linear(8192, 64), nn.BatchNorm1d(64), nn.ReLU(),\n","                           nn.Linear(64, 32), nn.BatchNorm1d(32), nn.ReLU(),\n","                           nn.Linear(32, num_classes))\n","classifier = classifier.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DjLZ6ZXq_CVQ","outputId":"71adecd2-ea19-4079-ad3e-51d67c940c39","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["conf, acc, wrong_predictions = eval()\n","print(acc)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor(7)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RPb3-WEb_CVg","outputId":"2aa997a4-d2c6-4948-bcf5-6cd36d9bfda2","colab":{"base_uri":"https://localhost:8080/","height":265}},"source":["plt.imshow(conf, cmap='jet', vmin=0, vmax = 1)\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAP8AAAD4CAYAAAAjDTByAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"FzV2QqgE_CVo"},"source":["for w in wrong_predictions[::10]:\n","    print(classes[w[2]], 'confused with', classes[w[1]])\n","    plt.imshow(w[0][0][0].data.cpu().numpy(), cmap='gray')\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uiwJOl40F0Uz"},"source":["### References\n","\n","1. https://hackernoon.com/one-shot-learning-with-siamese-networks-in-pytorch-8ddaab10340e\n","2. https://www.iflexion.com/blog/face-recognition-algorithms-work/\n","3. http://cvit.iiit.ac.in/projects/IMFDB/"]},{"cell_type":"markdown","metadata":{"id":"0JZfPfrSshl3"},"source":["### Please answer the questions below to complete the experiment:"]},{"cell_type":"code","metadata":{"id":"beeyqpHaGjmV"},"source":["#@title The Siamese data loader program used above, as usual, generates image-label (where the label is one of the class the image belongs to) pair? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n","Answer = \"False\" #@param [\"\",\"True\",\"False\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B0KcZCz5e6sA"},"source":["#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n","Complexity = \"Good and Challenging me\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZZTYl4pmg9Qq"},"source":["#@title If it was very easy, what more you would have liked to have been added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n","Additional = \"NA\" #@param {type:\"string\"}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4YKDp73pg_IE"},"source":["#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Concepts = \"Yes\" #@param [\"\",\"Yes\", \"No\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WxQbRbxDXXTA"},"source":["#@title  Experiment walkthrough video? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Walkthrough = \"Somewhat Useful\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GkRrtJn7XXa5"},"source":["#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Comments = \"Somewhat Useful\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g8VxMfNdXXWz"},"source":["#@title In class Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Inclass_support = \"Didn't use\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VEhnkd0hXXPy"},"source":["#@title Online Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Online_support = \"Didn't use\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"bJ72nhqosqhp","outputId":"44487314-e766-425f-ce0e-1630174b8662","colab":{"base_uri":"https://localhost:8080/","height":109}},"source":["#@title Run this cell to submit your notebook for grading { vertical-output: true }\n","try:\n","  if submission_id:\n","      return_id = submit_notebook()\n","      if return_id : submission_id =return_id\n","  else:\n","      print(\"Please complete the setup first.\")\n","except NameError:\n","  print (\"Please complete the setup first.\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Your submission is successful.\n","Ref Id: 17480\n","Date of submission:  03 Jan 2020\n","Time of submission:  00:02:47\n","View your submissions: https://iiith-aiml.talentsprint.com/notebook_submissions\n"],"name":"stdout"}]}]}